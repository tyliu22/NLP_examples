{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI - Building a Contextual Retrieval System for Improving RAG Accuracy\n",
    "\n",
    "For larger knowledge bases, Retrieval-Augmented Generation (RAG) offers a scalable solution. Modern RAG systems combine two powerful retrieval methods:\n",
    "\n",
    " - Semantic Search using Embeddings\n",
    " - Chunks the knowledge base into manageable segments (typically a few hundred tokens each)\n",
    " - Converts these chunks into vector embeddings that capture semantic meaning\n",
    " - Stores embeddings in a vector database for similarity searching\n",
    " - Lexical Search using BM25\n",
    " - Builds on TF-IDF (Term Frequency-Inverse Document Frequency) principles\n",
    " - Accounts for document length and term frequency saturation\n",
    " - Excels at finding exact matches and specific terminology\n",
    " \n",
    "\n",
    "The optimal RAG implementation combines both approaches:\n",
    "\n",
    " - Split the knowledge base into chunks\n",
    " - Generate both TF-IDF encodings and semantic embeddings\n",
    " - Run parallel searches using BM25 and embedding similarity\n",
    " - Merge and deduplicate results using rank fusion\n",
    " - Include the most relevant chunks in the prompt\n",
    " - Generate the response using the enhanced context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from rank_bm25 import BM25Okapi\n",
    "import cohere\n",
    "import logging\n",
    "import time\n",
    "from llama_parse import LlamaParse\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "from langchain_community.document_loaders.doc_intelligence import AzureAIDocumentIntelligenceLoader\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "load_dotenv('azure.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embedding\n",
    "\n",
    " - Uses Azure AI Document Intelligence for PDF parsing\n",
    " - Breaks documents into manageable chunks while maintaining context\n",
    " - Implements sophisticated text splitting with overlap to ensure no information is lost at chunk boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualRetrieval:\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=100,\n",
    "        )\n",
    "        self.embeddings = AzureOpenAIEmbeddings(\n",
    "                            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                            azure_deployment=\"text-embedding-ada-002\",\n",
    "                            openai_api_version=\"2024-03-01-preview\",\n",
    "                            azure_endpoint =os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "                        )\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            azure_deployment=\"gpt-4o\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "        self.cohere_client = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "    def load_pdf_and_parse(self, pdf_path: str) -> str:\n",
    "        loader = AzureAIDocumentIntelligenceLoader(file_path=pdf_path, \n",
    "                                           api_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"), \n",
    "                                           api_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\"),\n",
    "                                           api_model=\"prebuilt-layout\",\n",
    "                                           api_version=\"2024-02-29-preview\",\n",
    "                                           mode='markdown',\n",
    "                                           analysis_features = [DocumentAnalysisFeature.OCR_HIGH_RESOLUTION])\n",
    "\n",
    "        try:\n",
    "            documents = loader.load()\n",
    "            if not documents:\n",
    "                raise ValueError(\"No content extracted from the PDF.\")\n",
    "            return \" \".join([doc.page_content for doc in documents])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while parsing the file '{pdf_path}': {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_document(self, document: str) -> Tuple[List[Document], List[Document]]:\n",
    "        if not document.strip():\n",
    "            raise ValueError(\"The document is empty after parsing.\")\n",
    "        chunks = self.text_splitter.create_documents([document])\n",
    "        contextualized_chunks = self._generate_contextualized_chunks(document, chunks)\n",
    "        return chunks, contextualized_chunks\n",
    "\n",
    "    def _generate_contextualized_chunks(self, document: str, chunks: List[Document]) -> List[Document]:\n",
    "        contextualized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            context = self._generate_context(document, chunk.page_content)\n",
    "            contextualized_content = f\"{context}\\n\\n{chunk.page_content}\"\n",
    "            contextualized_chunks.append(Document(page_content=contextualized_content, metadata=chunk.metadata))\n",
    "        return contextualized_chunks\n",
    "\n",
    "    def _generate_context(self, document: str, chunk: str) -> str:\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant specializing in document analysis. Your task is to provide brief, relevant context for a chunk of text from the given document.\n",
    "        Here is the document:\n",
    "        <document>\n",
    "        {document}\n",
    "        </document>\n",
    "\n",
    "        Here is the chunk we want to situate within the whole document:\n",
    "        <chunk>\n",
    "        {chunk}\n",
    "        </chunk>\n",
    "\n",
    "        Provide a concise context (2-3 sentences) for this chunk, considering the following guidelines:\n",
    "        1. Identify the main topic or concept discussed in the chunk.\n",
    "        2. Mention any relevant information or comparisons from the broader document context.\n",
    "        3. If applicable, note how this information relates to the overall theme or purpose of the document.\n",
    "        4. Include any key figures, dates, or percentages that provide important context.\n",
    "        5. Do not use phrases like \"This chunk discusses\" or \"This section provides\". Instead, directly state the context.\n",
    "\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
    "\n",
    "        Context:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(document=document, chunk=chunk)\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    def create_bm25_index(self, chunks: List[Document]) -> BM25Okapi:\n",
    "        tokenized_chunks = [chunk.page_content.split() for chunk in chunks]\n",
    "        return BM25Okapi(tokenized_chunks)\n",
    "\n",
    "    def generate_answer(self, query: str, relevant_chunks: List[str]) -> str:\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Based on the following information, please provide a concise and accurate answer to the question.\n",
    "        If the information is not sufficient to answer the question, say so.\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Relevant information:\n",
    "        {chunks}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(query=query, chunks=\"\\n\\n\".join(relevant_chunks))\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    def rerank_results(self, query: str, documents: List[Document], top_n: int = 3) -> List[Document]:\n",
    "        logging.info(f\"Reranking {len(documents)} documents for query: {query}\")\n",
    "        doc_contents = [doc.page_content for doc in documents]\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                reranked = self.cohere_client.rerank(\n",
    "                    model=\"rerank-english-v2.0\",\n",
    "                    query=query,\n",
    "                    documents=doc_contents,\n",
    "                    top_n=top_n\n",
    "                )\n",
    "                break\n",
    "            except cohere.errors.TooManyRequestsError:\n",
    "                if attempt < max_retries - 1:\n",
    "                    logging.warning(f\"Rate limit hit. Waiting for 60 seconds before retry {attempt + 1}/{max_retries}\")\n",
    "                    time.sleep(60)  # Wait for 60 seconds before retrying\n",
    "                else:\n",
    "                    logging.error(\"Rate limit hit. Max retries reached. Returning original documents.\")\n",
    "                    return documents[:top_n]\n",
    "        \n",
    "        logging.info(f\"Reranking complete. Top {top_n} results:\")\n",
    "        reranked_docs = []\n",
    "        for idx, result in enumerate(reranked.results):\n",
    "            original_doc = documents[result.index]\n",
    "            reranked_docs.append(original_doc)\n",
    "            logging.info(f\"  {idx+1}. Score: {result.relevance_score:.4f}, Index: {result.index}\")\n",
    "        \n",
    "        return reranked_docs\n",
    "\n",
    "    def expand_query(self, original_query: str) -> str:\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant specializing in document analysis. Your task is to expand the given query to include related terms and concepts that might be relevant for a more comprehensive search of the document.\n",
    "\n",
    "        Original query: {query}\n",
    "\n",
    "        Please provide an expanded version of this query, including relevant terms, concepts, or related ideas that might help in summarizing the full document. The expanded query should be a single string, not a list.\n",
    "\n",
    "        Expanded query:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(query=original_query)\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset and contextual embedding\n",
    "\n",
    "Now lets load a sample PDF with Contextual embedding and create 2 index both for normal chunks and context aware chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = ContextualRetrieval()\n",
    "pdf_path = \"1.pdf\"\n",
    "document = cr.load_pdf_with_llama_parse(pdf_path)\n",
    "\n",
    "# Process the document\n",
    "chunks, contextualized_chunks = cr.process_document(document)\n",
    "\n",
    "# Create BM25 index\n",
    "contextualized_bm25_index = cr.create_bm25_index(contextualized_chunks)\n",
    "normal_bm25_index = cr.create_bm25_index(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the query against the both the index to compare the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str, processor: AutoProcessor, model: ColPali) -> np.ndarray:\n",
    "    mock_image = Image.new('RGB', (224, 224), color='white')\n",
    "\n",
    "    inputs = processor(text=query, images=mock_image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs)\n",
    "\n",
    "    return torch.mean(embeddings, dim=1).float().cpu().numpy().tolist()[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = \"When does the term of the Agreement commence and how long does it last?\"\n",
    "print(f\"\\nOriginal Query: {original_query}\")\n",
    "process_query(cr, original_query, normal_bm25_index, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create your ground truth based on the following jsonlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"chat_history\":[],\"question\":\"What is short-term memory in the context of the model?\",\"ground_truth\":\"Short-term memory involves utilizing in-context learning to learn.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(output_file, lines=True, orient=\"records\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_answers = []\n",
    "contexual_answers = []\n",
    "for index, row in df.iterrows():\n",
    "    normal_answers.append(process_query(cr, row[\"question\"], normal_bm25_index, chunks))\n",
    "    contexual_answers.append(process_query(cr, row[\"question\"], contextualized_bm25_index, contextualized_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate against the ground truth , here in this case i have used similarity score for evaluation. You can use any other builtin or custom metrics. Learn more about it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "\n",
    "df[\"answer\"] = normal_answers\n",
    "df['score'] = df.apply(lambda x : similarity_eval(\n",
    "    response=x[\"answer\"],\n",
    "    ground_truth = x[\"ground_truth\"],\n",
    "    query=x[\"question\"],\n",
    "), axis = 1)\n",
    "df[\"answer_contextual\"] = contexual_answers\n",
    "df['score_contextual'] = df.apply(lambda x : similarity_eval(\n",
    "    response=x[\"answer_contextual\"],\n",
    "    ground_truth = x[\"ground_truth\"],\n",
    "    query=x[\"question\"],\n",
    "), axis = 1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
