{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crayon RAG based chatbot system - RAG Chatbot using ChromaDB, LlamaIndex and Gradio\n",
    "\n",
    " - Environment Setup \n",
    " - Data EDA\n",
    "    - Loading data\n",
    "    - Data exploration\n",
    " - Building RAG system and vector database\n",
    "    - split documents into chunks\n",
    "    - embedding chunks into vector and store vectors into local directory\n",
    " - Building RAG based ChatBot with WebUI and LLM function calling \n",
    " - Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setting\n",
    "\n",
    "Configure the environment variables and python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex version: 0.11.18\n",
      "Weaviate version: 0.5.16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import llama_index\n",
    "import chromadb  # vector database\n",
    "import gradio as gr  # WebUI\n",
    "from openai import OpenAI  # OpenAI\n",
    "from importlib.metadata import version\n",
    "\n",
    "# Loading environment variables\n",
    "from dotenv import load_dotenv,find_dotenv  \n",
    "\n",
    "# LlamaIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Verify the LlamaIndex version and Vector database version\n",
    "print(f\"LlamaIndex version: {version('llama_index')}\")\n",
    "print(f\"chromadb version: {version('chromadb')}\")\n",
    "\n",
    "# Use this line of code if you have a local .env file\n",
    "load_dotenv(find_dotenv()) \n",
    "# Settings LLM in LlamaIndex  --->  llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data ETA\n",
    "\n",
    "- Loading the dataset\n",
    "- Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "documents = SimpleDirectoryReader(\"./dataset/policy\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Building RAG system\n",
    "Building the RAG system consists of seveal steps:\n",
    "- Split documents into chunks (or Nodes)\n",
    "- Embedding chunks into vectors and sotre vectors into Vector Database\n",
    "- Create a retriever using vector database\n",
    "- Semantic search using query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 : Chunk documents into Nodes\n",
    "\n",
    "As the whole document is too large to fit into the context window of the LLM, you will need to partition it into smaller text chunks, which are called Nodes in LlamaIndex.\n",
    "\n",
    "With the SimpleNodeParser each document is stored as chunks. Each chunk consists of 258 tokens. Different chunks are overlapeed with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split documents into chunks\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=258, chunk_overlap=50)\n",
    "# Extract nodes from documents\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Embedding chunks into vectors and sotre vectors into Vector Database via cheromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./dataset/chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"policy_knowledge\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create your index\n",
    "# build VectorStoreIndex that takes care of chunking documents and encoding chunks to embeddings for future retrieval\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Create a retriever using Chroma \n",
    "creata a retriever to support allows for semantic search\n",
    "\n",
    "First, initialize the PersistentClient with the same path you specified while creating the Chroma vector store. You'll then retrieve the collection \"policy_knowledge\" you created previously from Chroma. You can use this collection to initialize the ChromaVectorStore in which you store the embeddings of the website data. You can then use the from_vector_store function of VectorStoreIndex to load the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "load_client = chromadb.PersistentClient(path=\"./dataset/chroma_db\")\n",
    "\n",
    "# Fetch the collection\n",
    "chroma_collection = load_client.get_collection(\"policy_knowledge\")\n",
    "\n",
    "# Fetch the vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Get the index from the vector store\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Semantic search with vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context information does not mention anything about government financial policy.\n"
     ]
    }
   ],
   "source": [
    "def get_external_policy_knowledge(query):\n",
    "    # This funciton is for serching the relevant chunks via semantic search\n",
    "    # and get responce based on the LLM\n",
    "\n",
    "    # check if the retriever is working by trying to fetch the relevant docs related\n",
    "    test_query_engine = index.as_query_engine()\n",
    "    response = test_query_engine.query(query)\n",
    "    return str(response)  # Convert response to string to ensure it's serializable\n",
    "\n",
    "\n",
    "# Text the query semantic search and get the answer based on the OPENAI 3.5 \n",
    "print(get_external_policy_knowledge(\"Tell me about the government fiancial policy\"))\n",
    "\n",
    "\n",
    "def get_relevant_chunks(query):\n",
    "    # This funciton is for serching the relevant chunks via semantic search\n",
    "    retriever = VectorIndexRetriever(index=index, similarity_top_k=1,)\n",
    "    #Finding out the nodes for the new query:1``\n",
    "    nodes=retriever.retrieve(query)\n",
    "    return nodes[0].text\n",
    "    # print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Chatbot - OpenAI with function calling\n",
    "Building RAG based ChatBot with WebUI and LLM function calling\n",
    " - Use the LLM with function calling as the router to support multi-turn conversation\n",
    " - Build the WebUI via gradio as a prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function schema that OpenAI will use\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_external_policy_knowledge\",\n",
    "        \"description\": \"Retrieve knowledge from external policy documents. This documents database consits of Data Privacy Policy, AI Ethics Policy and Model Governance Policy\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The query to search for in the policy documents\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyuliu/Library/Caches/pypoetry/virtualenvs/nlp-examples-2S-piTS9-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Develop a comprehensive ethical risk assessment toolkit that includes templates, best practices, and guidelines to standardize the assessment process across the company.\n",
      "The document does not provide information on government financial policy.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)\n",
    "\n",
    "def predict(message, history):\n",
    "    history_openai_format = [{\"role\": \"system\", \"content\": \"You are AI assistant, only answer the question based on the chat history or the information extracted from the external database. Don't answer questions based on you own knowledge.\"}]\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human})\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # First, use the LLM as the query router to justify if LLM direct give the answer OR call the function to retrieve the relevant konwledge from vector database\n",
    "    # get the response from OpenAI with potential function calls\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=history_openai_format,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "\n",
    "    # Check if the model wants to call a function\n",
    "    if response_message.function_call:\n",
    "        # Get the function call details\n",
    "        function_name = response_message.function_call.name\n",
    "        function_args = json.loads(response_message.function_call.arguments)\n",
    "\n",
    "        # Call the function\n",
    "        if function_name == \"get_external_policy_knowledge\":\n",
    "            function_response = get_external_policy_knowledge(function_args.get(\"query\"))\n",
    "            print(function_response)\n",
    "\n",
    "            return function_response\n",
    "\n",
    "            # # Add the function result to the message history\n",
    "            # history_openai_format.append({\n",
    "            #     \"role\": \"function\",\n",
    "            #     \"name\": function_name,\n",
    "            #     \"content\": function_response\n",
    "            # })\n",
    "\n",
    "            # # Get a new response from the model with the function result\n",
    "            # second_response = client.chat.completions.create(\n",
    "            #     model='gpt-4',\n",
    "            #     messages=history_openai_format,\n",
    "            #     temperature=0.1\n",
    "            # )\n",
    "            \n",
    "            # return second_response.choices[0].message.content\n",
    "    else:\n",
    "        # If no function call is needed, return the response directly\n",
    "        return response_message.content\n",
    "\n",
    "\n",
    "# Create and launch the Gradio WebUI interface\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model evaluation\n",
    "### Eval 1: LLM as a router performance \n",
    " - confusion matrix\n",
    "\n",
    " - **Test data**: both in-domain and out of domain question\n",
    "\n",
    "### Eval 1: RAG system perofrmance\n",
    " - Accuracy\n",
    "   - BLEU or ROUGE\n",
    " - Fathfulness\n",
    " - RAG retriever relevance\n",
    " - Latency \n",
    "    - LLM responce time\n",
    "    - RAG responces time\n",
    "\n",
    " - **Test data:** in-domain question which is relevant to the documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Define several labeled text cases\n",
    "Manully provide several test cases with corresponding answers:\n",
    "In the test datset, we provides 5 test cases, which consists of \n",
    "- In-domian questions \n",
    "- Irrelevant question \n",
    "- Out of distribution questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_text_cases = [\n",
    "    # In-domain questions in terms of data policy, AI policy and model policy\n",
    "    {   # Data pirvacy policy question \n",
    "        \"text\": \"Where should the data be stored.\",\n",
    "        \"expected_answer\": \"Data is securely stored in state-of-the-art data centers located in the United States, the European Union, and other jurisdictions, depending on the nature of the data and the services provided. Each location is chosen based on stringent security standards and data protection compliance.\",\n",
    "        \"label\": \"function_call\"\n",
    "    },\n",
    "    {   # AI ethics policy question \n",
    "        \"text\": \"Tell me about the AI Transparency rules.\",\n",
    "        \"expected_answer\": \"Enhance transparency by developing interfaces that allow users to query AI decisions and receive explanations in understandable terms. Document all AI systems decision-making processes and methodologies, ensuring that this documentation is accessible to all relevant stakeholders and regularly updated.\",\n",
    "        \"label\": \"function_call\"\n",
    "    },\n",
    "    {   # Model governance policy question \n",
    "        \"text\": \"How to test and validate the model?\",\n",
    "        \"expected_answer\": \"Models must undergo rigorous testing to validate their accuracy, performance, and fairness. Validation tests should be designed to cover various operational scenarios and should include stress and failure mode analysis. Documentation of all test results is mandatory for auditability and further review.\",\n",
    "        \"label\": \"function_call\"\n",
    "    },\n",
    "\n",
    "    # Out of distribution question\n",
    "    {   # Qauestion from other domains\n",
    "        \"text\": \"Tell me about the Australia financial policy\",\n",
    "        \"expected_answer\": \"Albert Einstein\",\n",
    "        \"label\": \"direct_answer\"\n",
    "    },\n",
    "\n",
    "    # Irrelevant question\n",
    "    {   # general question. No necessary to call the RAG system \n",
    "        \"text\": \"Hi.\",\n",
    "        \"expected_answer\": \"Hello, what can I help you today\",\n",
    "        \"label\": \"direct_answer\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Eval LLM as a Router\n",
    "    Check if the LLM can call the RAG system precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where should the data be stored.\n",
      "direct_answer\n",
      "Tell me about the AI Transparency rules.\n",
      "direct_answer\n",
      "How to test and validate the model?\n",
      "direct_answer\n",
      "Tell me about the Australia financial policy\n",
      "direct_answer\n",
      "Hi.\n",
      "direct_answer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)\n",
    "\n",
    "def indicator_call_RAG(query):\n",
    "    # providing no response is more acceptable than offering an incorrect one, as the latter could mislead the user regarding their query\n",
    "    # history_openai_format = [{\"role\": \"system\", \"content\": \"You are AI assistant, only answer the question based on the chat history or the information extracted from the external database. Don't answer questions based on you own knowledge.\"}]\n",
    "    history_openai_format = [{\"role\": \"system\", \"content\": \"You are a policy documents assistant, and you are responsible to answer any data pirvacy, AI ethics and data governance policy questions. You can provid no response as it is more acceptable than offering an incorrect one, as the latter could mislead the user regarding their query.\"},\n",
    "                             {\"role\": \"system\", \"content\": \"{query}\"}\n",
    "                             ]\n",
    "\n",
    "    # First, use the LLM as the query router to justify if LLM direct give the answer OR call the function to retrieve the relevant konwledge from vector database\n",
    "    # get the response from OpenAI with potential function calls\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=history_openai_format,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "\n",
    "    # Check if the model wants to call a function\n",
    "    if response_message.function_call:\n",
    "        return \"function_call\"\n",
    "    else:\n",
    "        return \"direct_answer\"\n",
    "\n",
    "for item in labeled_text_cases:\n",
    "    print(item[\"text\"])\n",
    "    item[\"predicted_label\"] = indicator_call_RAG(item[\"text\"])\n",
    "    print(item[\"predicted_label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct_answer\n"
     ]
    }
   ],
   "source": [
    "print(indicator_call_RAG(\"AI risk assessment\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-examples-2S-piTS9-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
