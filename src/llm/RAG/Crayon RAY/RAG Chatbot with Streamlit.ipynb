{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crayon RAG based chatbot system - RAG Chatbot using ChromaDB, LlamaIndex and Gradio\n",
    "\n",
    " - Environment Setup \n",
    " - Data EDA\n",
    "    - Loading data\n",
    "    - Data exploration\n",
    " - Building RAG system and vector database\n",
    "    - split documents into chunks\n",
    "    - embedding chunks into vector and store vectors into local directory\n",
    " - Building RAG based ChatBot with WebUI and LLM function calling \n",
    " - Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setting\n",
    "\n",
    "Configure the environment variables and python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex version: 0.11.18\n",
      "chromadb version: 0.5.16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import llama_index\n",
    "import chromadb  # vector database\n",
    "import gradio as gr  # WebUI\n",
    "from openai import OpenAI  # OpenAI\n",
    "from importlib.metadata import version\n",
    "\n",
    "# Loading environment variables\n",
    "from dotenv import load_dotenv,find_dotenv  \n",
    "\n",
    "# LlamaIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Verify the LlamaIndex version and Vector database version\n",
    "print(f\"LlamaIndex version: {version('llama_index')}\")\n",
    "print(f\"chromadb version: {version('chromadb')}\")\n",
    "\n",
    "# Use this line of code if you have a local .env file\n",
    "load_dotenv(find_dotenv()) \n",
    "# Settings LLM in LlamaIndex  --->  llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data ETA\n",
    "\n",
    "- Loading the dataset\n",
    "- Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "documents = SimpleDirectoryReader(\"./dataset/policy\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Building RAG system\n",
    "Building the RAG system consists of seveal steps:\n",
    "- Split documents into chunks (or Nodes)\n",
    "- Embedding chunks into vectors and sotre vectors into Vector Database\n",
    "- Create a retriever using vector database\n",
    "- Semantic search using query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 : Chunk documents into Nodes\n",
    "\n",
    "As the whole document is too large to fit into the context window of the LLM, you will need to partition it into smaller text chunks, which are called Nodes in LlamaIndex.\n",
    "\n",
    "With the SimpleNodeParser each document is stored as chunks. Each chunk consists of 258 tokens. Different chunks are overlapeed with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split documents into chunks\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=258, chunk_overlap=50)\n",
    "# Extract nodes from documents\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Embedding chunks into vectors and sotre vectors into Vector Database via cheromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./dataset/chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"policy_knowledge\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create your index\n",
    "# build VectorStoreIndex that takes care of chunking documents and encoding chunks to embeddings for future retrieval\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Create a retriever using Chroma \n",
    "creata a retriever to support allows for semantic search\n",
    "\n",
    "First, initialize the PersistentClient with the same path you specified while creating the Chroma vector store. You'll then retrieve the collection \"policy_knowledge\" you created previously from Chroma. You can use this collection to initialize the ChromaVectorStore in which you store the embeddings of the website data. You can then use the from_vector_store function of VectorStoreIndex to load the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "load_client = chromadb.PersistentClient(path=\"./dataset/chroma_db\")\n",
    "\n",
    "# Fetch the collection\n",
    "chroma_collection = load_client.get_collection(\"policy_knowledge\")\n",
    "\n",
    "# Fetch the vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Get the index from the vector store\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Semantic search with vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context information does not mention anything about government financial policy.\n"
     ]
    }
   ],
   "source": [
    "def get_external_policy_knowledge(query):\n",
    "    # This funciton is for serching the relevant chunks via semantic search\n",
    "    # and get responce based on the LLM\n",
    "\n",
    "    # check if the retriever is working by trying to fetch the relevant docs related\n",
    "    test_query_engine = index.as_query_engine()\n",
    "    response = test_query_engine.query(query)\n",
    "    return str(response)  # Convert response to string to ensure it's serializable\n",
    "\n",
    "\n",
    "# Text the query semantic search and get the answer based on the OPENAI 3.5 \n",
    "print(get_external_policy_knowledge(\"Tell me about the government fiancial policy\"))\n",
    "\n",
    "\n",
    "def get_relevant_chunks(query):\n",
    "    # This funciton is for serching the relevant chunks via semantic search\n",
    "    retriever = VectorIndexRetriever(index=index, similarity_top_k=1,)\n",
    "    #Finding out the nodes for the new query:1``\n",
    "    nodes=retriever.retrieve(query)\n",
    "    return nodes[0].text\n",
    "    # print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Chatbot - OpenAI with function calling\n",
    "Building RAG based ChatBot with WebUI and LLM function calling\n",
    " - Use the LLM with function calling as the router to support multi-turn conversation\n",
    " - Build the WebUI via gradio as a prototype\n",
    " - gpt-4 as the LLM router\n",
    " - gpt-3.5-turbo is the answer summarizer to summarize the query and extracted text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function schema that OpenAI will use\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_external_policy_knowledge\",\n",
    "        \"description\": \"Retrieve knowledge from external policy documents. This documents database consits of Data Privacy Policy, AI Ethics Policy and Model Governance Policy\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The query to search for in the policy documents\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyuliu/Library/Caches/pypoetry/virtualenvs/nlp-examples-2S-piTS9-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Develop a comprehensive ethical risk assessment toolkit that includes templates, best practices, and guidelines to standardize the assessment process across the company.\n",
      "The document does not provide information on government financial policy.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)\n",
    "\n",
    "def predict(message, history):\n",
    "    history_openai_format = [{\"role\": \"system\", \"content\": \"You are AI assistant, only answer the question based on the chat history or the information extracted from the external database. Don't answer questions based on you own knowledge.\"}]\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human})\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # First, use the LLM as the query router to justify if LLM direct give the answer OR call the function to retrieve the relevant konwledge from vector database\n",
    "    # get the response from OpenAI with potential function calls\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=history_openai_format,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "\n",
    "    # Check if the model wants to call a function\n",
    "    if response_message.function_call:\n",
    "        # Get the function call details\n",
    "        function_name = response_message.function_call.name\n",
    "        function_args = json.loads(response_message.function_call.arguments)\n",
    "\n",
    "        # Call the function\n",
    "        if function_name == \"get_external_policy_knowledge\":\n",
    "            function_response = get_external_policy_knowledge(function_args.get(\"query\"))\n",
    "            print(function_response)\n",
    "\n",
    "            return function_response\n",
    "\n",
    "            # # Add the function result to the message history\n",
    "            # history_openai_format.append({\n",
    "            #     \"role\": \"function\",\n",
    "            #     \"name\": function_name,\n",
    "            #     \"content\": function_response\n",
    "            # })\n",
    "\n",
    "            # # Get a new response from the model with the function result\n",
    "            # second_response = client.chat.completions.create(\n",
    "            #     model='gpt-4',\n",
    "            #     messages=history_openai_format,\n",
    "            #     temperature=0.1\n",
    "            # )\n",
    "            \n",
    "            # return second_response.choices[0].message.content\n",
    "    else:\n",
    "        # If no function call is needed, return the response directly\n",
    "        return response_message.content\n",
    "\n",
    "\n",
    "# Create and launch the Gradio WebUI interface\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model evaluation\n",
    "### Eval 1: LLM as a router performance \n",
    " - confusion matrix\n",
    "\n",
    " - **Test data**: both in-domain and out of domain question\n",
    "\n",
    "### Eval 2: RAG system perofrmance\n",
    " - Accuracy\n",
    "   - BLEU or ROUGE\n",
    " - Faithfulness\n",
    " - RAG retriever relevance\n",
    " - Latency \n",
    "    - LLM response time\n",
    "    - RAG responses time\n",
    "\n",
    " - **Test data:** in-domain question which is relevant to the documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Define several labeled text cases\n",
    "Manully provide several test cases with corresponding answers:\n",
    "In the test datset, we provides 5 test cases, which consists of \n",
    "- In-domian questions \n",
    "- Irrelevant question \n",
    "- Out of distribution questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_text_cases = [\n",
    "    # In-domain questions in terms of data policy, AI policy and model policy\n",
    "    {   # Data pirvacy policy question \n",
    "        \"text\": \"Where should the data be stored.\",\n",
    "        \"expected_answer\": \"Data is securely stored in state-of-the-art data centers located in the United States, the European Union, and other jurisdictions, depending on the nature of the data and the services provided. Each location is chosen based on stringent security standards and data protection compliance.\",\n",
    "        \"label\": \"function_call\"\n",
    "    },\n",
    "    {   # AI ethics policy question \n",
    "        \"text\": \"Tell me about the AI Transparency rules.\",\n",
    "        \"expected_answer\": \"Enhance transparency by developing interfaces that allow users to query AI decisions and receive explanations in understandable terms. Document all AI systems decision-making processes and methodologies, ensuring that this documentation is accessible to all relevant stakeholders and regularly updated.\",\n",
    "        \"label\": \"function_call\"\n",
    "    },\n",
    "    {   # Model governance policy question \n",
    "        \"text\": \"How to test and validate the model?\",\n",
    "        \"expected_answer\": \"Models must undergo rigorous testing to validate their accuracy, performance, and fairness. Validation tests should be designed to cover various operational scenarios and should include stress and failure mode analysis. Documentation of all test results is mandatory for auditability and further review.\",\n",
    "        \"label\": \"function_call\"\n",
    "    },\n",
    "\n",
    "    # Out of distribution question\n",
    "    {   # Qauestion from other domains\n",
    "        \"text\": \"Tell me about the Australia financial regulation\",\n",
    "        \"expected_answer\": \"I cannot answer this question\",\n",
    "        \"label\": \"direct_answer\"\n",
    "    },\n",
    "\n",
    "    # Irrelevant question\n",
    "    {   # general question. No necessary to call the RAG system \n",
    "        \"text\": \"Hi.\",\n",
    "        \"expected_answer\": \"Hello, what can I help you today\",\n",
    "        \"label\": \"direct_answer\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Eval LLM as a Router\n",
    "    Check if the LLM can call the RAG system precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)\n",
    "\n",
    "def indicator_call_RAG(query):\n",
    "    # providing no response is more acceptable than offering an incorrect one, as the latter could mislead the user regarding their query\n",
    "    # history_openai_format = [{\"role\": \"system\", \"content\": \"You are AI assistant, only answer the question based on the chat history or the information extracted from the external database. Don't answer questions based on you own knowledge.\"}]\n",
    "    history_openai_format = [{\"role\": \"system\", \"content\": \"You are a policy documents assistant, and you are responsible to answer company data pirvacy, AI ethics and data governance policy questions. You can provid no response as it is more acceptable than offering an incorrect one, as the latter could mislead the user regarding their query.\"}]\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": query})\n",
    "    # First, use the LLM as the query router to justify if LLM direct give the answer OR call the function to retrieve the relevant konwledge from vector database\n",
    "    # get the response from OpenAI with potential function calls\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=history_openai_format,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "\n",
    "    # Check if the model wants to call a function\n",
    "    if response_message.function_call:\n",
    "        return \"function_call\"\n",
    "    else:\n",
    "        return \"direct_answer\"\n",
    "\n",
    "for item in labeled_text_cases:\n",
    "    item[\"predicted_label\"] = indicator_call_RAG(item[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "               function_call  direct_answer\n",
      "function_call              3              0\n",
      "direct_answer              1              1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Extract labels and predicted labels\n",
    "y_true = [item['label'] for item in labeled_text_cases]\n",
    "y_pred = [item['predicted_label'] for item in labeled_text_cases]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "labels = list(set(y_true + y_pred))  # Get unique labels\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: RAG system peroformance\n",
    " - Accuracy\n",
    "   - BLEU or ROUGE\n",
    " - Faithfulness\n",
    " - RAG retriever relevance\n",
    " - Latency \n",
    "    - LLM response time\n",
    "    - RAG responses time\n",
    "\n",
    " - **Test data:** in-domain question which is relevant to the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyuliu/Library/Caches/pypoetry/virtualenvs/nlp-examples-2S-piTS9-py3.12/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/tianyuliu/Library/Caches/pypoetry/virtualenvs/nlp-examples-2S-piTS9-py3.12/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'accuracy': {'avg_bleu': 0.6449922429023552, 'avg_rouge': {'rouge-1': 0.7373737323752678, 'rouge-2': 0.6558307483542042, 'rouge-l': 0.7272727222742578}}, 'faithfulness': 0.6541802883148193, 'retriever_relevance': 0.0, 'latency': {'avg_retriever_time': 0.6374595165252686, 'avg_llm_time': 2.9588892459869385}}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import time\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Load the embedding model (e.g., SentenceTransformer)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and fast for semantic similarity\n",
    "\n",
    "def evaluate_rag_system_with_semantics(labeled_text_cases):\n",
    "    \"\"\"\n",
    "    Evaluate the RAG system based on the provided labeled cases, with semantic analysis for faithfulness.\n",
    "\n",
    "    Metrics:\n",
    "    - Accuracy: BLEU, ROUGE\n",
    "    - Faithfulness: Semantic similarity between retrieved chunks and LLM response\n",
    "    - RAG retriever relevance: Compare retrieved chunks with expected\n",
    "    - Latency: Measure response time for retriever and LLM\n",
    "\n",
    "    Args:\n",
    "        labeled_text_cases (list): A list of test cases with queries, expected answers, and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics results.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"accuracy\": {\"bleu\": [], \"rouge\": []},\n",
    "        \"faithfulness\": [],\n",
    "        \"retriever_relevance\": [],\n",
    "        \"latency\": {\"retriever_time\": [], \"llm_time\": []},\n",
    "    }\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    labeled_text_cases = labeled_text_cases[:3]\n",
    "\n",
    "    for case in labeled_text_cases:\n",
    "        query = case[\"text\"]\n",
    "        expected_answer = case[\"expected_answer\"]\n",
    "\n",
    "        # Measure retriever latency\n",
    "        retriever_start = time.time()\n",
    "        retrieved_chunks = get_relevant_chunks(query)\n",
    "        retriever_end = time.time()\n",
    "        metrics[\"latency\"][\"retriever_time\"].append(retriever_end - retriever_start)\n",
    "\n",
    "        # Measure LLM latency\n",
    "        llm_start = time.time()\n",
    "        llm_response = get_external_policy_knowledge(query)\n",
    "        llm_end = time.time()\n",
    "        metrics[\"latency\"][\"llm_time\"].append(llm_end - llm_start)\n",
    "\n",
    "        # Accuracy: BLEU and ROUGE\n",
    "        bleu_score = sentence_bleu([expected_answer.split()], llm_response.split())\n",
    "        rouge_scores = rouge.get_scores(llm_response, expected_answer, avg=True)\n",
    "        metrics[\"accuracy\"][\"bleu\"].append(bleu_score)\n",
    "        metrics[\"accuracy\"][\"rouge\"].append(rouge_scores)\n",
    "\n",
    "        # Faithfulness: Semantic similarity between retrieved chunks and LLM response\n",
    "        retrieved_embedding = embedding_model.encode(retrieved_chunks, convert_to_tensor=True)\n",
    "        llm_response_embedding = embedding_model.encode(llm_response, convert_to_tensor=True)\n",
    "        faithfulness_score = util.pytorch_cos_sim(retrieved_embedding, llm_response_embedding).item()\n",
    "        metrics[\"faithfulness\"].append(faithfulness_score)\n",
    "\n",
    "        # RAG retriever relevance: Check if retrieved chunks are relevant to the query\n",
    "        retriever_relevance = retrieved_chunks in expected_answer\n",
    "        metrics[\"retriever_relevance\"].append(retriever_relevance)\n",
    "\n",
    "    # Summarize results\n",
    "    results = {\n",
    "        \"accuracy\": {\n",
    "            \"avg_bleu\": sum(metrics[\"accuracy\"][\"bleu\"]) / len(metrics[\"accuracy\"][\"bleu\"]),\n",
    "            \"avg_rouge\": {\n",
    "                key: sum([score[key][\"f\"] for score in metrics[\"accuracy\"][\"rouge\"]]) / len(metrics[\"accuracy\"][\"rouge\"])\n",
    "                for key in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]\n",
    "            },\n",
    "        },\n",
    "        \"faithfulness\": np.mean(metrics[\"faithfulness\"]),\n",
    "        \"retriever_relevance\": sum(metrics[\"retriever_relevance\"]) / len(metrics[\"retriever_relevance\"]),\n",
    "        \"latency\": {\n",
    "            \"avg_retriever_time\": sum(metrics[\"latency\"][\"retriever_time\"]) / len(metrics[\"latency\"][\"retriever_time\"]),\n",
    "            \"avg_llm_time\": sum(metrics[\"latency\"][\"llm_time\"]) / len(metrics[\"latency\"][\"llm_time\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_rag_system_with_semantics(labeled_text_cases)\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLEU</td>\n",
       "      <td>0.644992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROUGE-1</td>\n",
       "      <td>0.737374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROUGE-2</td>\n",
       "      <td>0.655831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ROUGE-L</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>0.654180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Retriever Relevance</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avg Retriever Time (s)</td>\n",
       "      <td>0.637460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Avg LLM Time (s)</td>\n",
       "      <td>2.958889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric     Value\n",
       "0                    BLEU  0.644992\n",
       "1                 ROUGE-1  0.737374\n",
       "2                 ROUGE-2  0.655831\n",
       "3                 ROUGE-L  0.727273\n",
       "4            Faithfulness  0.654180\n",
       "5     Retriever Relevance  0.000000\n",
       "6  Avg Retriever Time (s)  0.637460\n",
       "7        Avg LLM Time (s)  2.958889"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def format_results_as_table(results):\n",
    "    \"\"\"\n",
    "    Format the RAG evaluation results into a table (Pandas DataFrame).\n",
    "\n",
    "    Args:\n",
    "        results (dict): The evaluation results.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A formatted table of results.\n",
    "    \"\"\"\n",
    "    # Convert the nested results into a flat structure for better readability\n",
    "    table_data = {\n",
    "        \"Metric\": [\"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"Faithfulness\", \"Retriever Relevance\", \"Avg Retriever Time (s)\", \"Avg LLM Time (s)\"],\n",
    "        \"Value\": [\n",
    "            results[\"accuracy\"][\"avg_bleu\"],\n",
    "            results[\"accuracy\"][\"avg_rouge\"][\"rouge-1\"],\n",
    "            results[\"accuracy\"][\"avg_rouge\"][\"rouge-2\"],\n",
    "            results[\"accuracy\"][\"avg_rouge\"][\"rouge-l\"],\n",
    "            results[\"faithfulness\"],\n",
    "            results[\"retriever_relevance\"],\n",
    "            results[\"latency\"][\"avg_retriever_time\"],\n",
    "            results[\"latency\"][\"avg_llm_time\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the table data\n",
    "    df_results = pd.DataFrame(table_data)\n",
    "    return df_results\n",
    "# Reformat the results into a table\n",
    "df_evaluation_results = format_results_as_table(results)\n",
    "\n",
    "df_evaluation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-examples-2S-piTS9-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
